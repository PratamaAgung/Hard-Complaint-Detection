{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_hard_complaint_rule(chat):\n",
    "    chat = str(chat)\n",
    "    disappointed_words = [\"kecewa\", \"tidak puas\", \"kurang puas\", \"brengsek\", \"bangsat\", \"brgsk\", \"bgst\", \"anjing\"]\n",
    "    for dw in disappointed_words:\n",
    "        if dw in chat:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_opening = []\n",
    "with open(\"opening_chat_for_fasttext.csv\") as file:\n",
    "    readCSV = csv.reader(file)\n",
    "    for row in readCSV:\n",
    "        chat = row[0].lower()\n",
    "        chat = re.sub(r\"\\.|\\?|,|#|!|\\n\", \"\", chat)\n",
    "        chat_opening.append(chat.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "size_output = 256\n",
    "model = FastText(chat_opening, size= size_output, window= 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"complaint_data_2k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data = np.array(data['Chat'].values.tolist())\n",
    "chat_label = np.array(data['Label'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chat in enumerate(chat_data):\n",
    "    chat = chat.lower()\n",
    "    chat = re.sub(r\"\\.|\\?|,|#|!|\\n\", \"\", chat)\n",
    "    chat = re.sub(r\"[A-Z]*[0-9]+[A-Z]*\", \"\", chat)\n",
    "    chat_data[i] = chat\n",
    "    \n",
    "vect = TfidfVectorizer()\n",
    "vect.fit(chat_data)\n",
    "feature_names = vect.get_feature_names()\n",
    "feature_map = {feat:idx for idx, feat in enumerate(feature_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(sentence):\n",
    "    return re.findall(r\"[\\w']+\", sentence)\n",
    "\n",
    "def oov_handler(word_list, fasttext, word_dict):\n",
    "    return_list = []\n",
    "    for w in word_list:\n",
    "        if w not in word_dict:\n",
    "            try:\n",
    "                most_similar = fasttext.wv.most_similar(positive=[w])[0]\n",
    "                if most_similar[1] > 0.9:\n",
    "                    return_list.append(most_similar[0])\n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            return_list.append(w)\n",
    "    return return_list\n",
    "\n",
    "def sentence_maker(word_list):\n",
    "    sentence = \"\"\n",
    "    for w in word_list:\n",
    "        sentence += w + \" \"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_encoded = []\n",
    "for chat in chat_data:\n",
    "    encoded_words = []\n",
    "    for w in chat.split(' '):\n",
    "        if len(w) > 1:\n",
    "            try:\n",
    "                encoded_words.append(model.wv[w])\n",
    "            except:\n",
    "                continue\n",
    "    if len(encoded_words) > 0:\n",
    "        chat_encoded.append(np.mean(encoded_words, axis= 0))\n",
    "    else:\n",
    "        chat_encoded.append(np.zeros(size_output))\n",
    "\n",
    "chat_encoded = np.array(chat_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "chat_tf_idf = []\n",
    "for chat in chat_data:\n",
    "    encoded_words = []\n",
    "    weights = []\n",
    "    \n",
    "    word_list = oov_handler(splitter(chat), model, feature_map.keys())\n",
    "    tf_idf_vect = vect.transform([sentence_maker(word_list)])\n",
    "    for w in set(word_list):\n",
    "        try:\n",
    "            weight, enc = tf_idf_vect[0, feature_map[w]], model.wv[w] \n",
    "            weights.append(weight)\n",
    "            encoded_words.append(enc)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    if len(encoded_words) > 0:\n",
    "        chat_tf_idf.append(np.average(encoded_words, axis= 0, weights= weights))\n",
    "    else:\n",
    "        chat_tf_idf.append(np.zeros(size_output))\n",
    "\n",
    "chat_tf_idf = np.array(chat_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits= 10, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean (not using weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.83       112\n",
      "           1       0.77      0.84      0.81        89\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       201\n",
      "   macro avg       0.82      0.82      0.82       201\n",
      "weighted avg       0.82      0.82      0.82       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88        91\n",
      "           1       0.90      0.91      0.90       110\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       201\n",
      "   macro avg       0.89      0.89      0.89       201\n",
      "weighted avg       0.90      0.90      0.90       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91       115\n",
      "           1       0.87      0.88      0.88        86\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       201\n",
      "   macro avg       0.89      0.89      0.89       201\n",
      "weighted avg       0.90      0.90      0.90       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.88       109\n",
      "           1       0.82      0.91      0.87        92\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       201\n",
      "   macro avg       0.87      0.87      0.87       201\n",
      "weighted avg       0.88      0.87      0.87       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86       101\n",
      "           1       0.85      0.88      0.87       100\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       201\n",
      "   macro avg       0.87      0.87      0.87       201\n",
      "weighted avg       0.87      0.87      0.87       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       101\n",
      "           1       0.86      0.83      0.85       100\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       201\n",
      "   macro avg       0.85      0.85      0.85       201\n",
      "weighted avg       0.85      0.85      0.85       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       105\n",
      "           1       0.85      0.85      0.85        96\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       201\n",
      "   macro avg       0.86      0.86      0.86       201\n",
      "weighted avg       0.86      0.86      0.86       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.81      0.86       114\n",
      "           1       0.78      0.90      0.83        86\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       200\n",
      "   macro avg       0.84      0.85      0.84       200\n",
      "weighted avg       0.85      0.84      0.85       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       116\n",
      "           1       0.78      0.83      0.80        84\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       200\n",
      "   macro avg       0.83      0.83      0.83       200\n",
      "weighted avg       0.83      0.83      0.83       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.82      0.87       102\n",
      "           1       0.83      0.93      0.88        98\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       200\n",
      "   macro avg       0.88      0.88      0.87       200\n",
      "weighted avg       0.88      0.88      0.87       200\n",
      "\n",
      "Average f1-score: 0.8609701492537314\n"
     ]
    }
   ],
   "source": [
    "f1_score = []\n",
    "for train_idx, test_idx in kf.split(chat_data):\n",
    "    train_x, test_x = chat_encoded[train_idx], chat_encoded[test_idx]\n",
    "    train_y, test_y = chat_label[train_idx], chat_label[test_idx]\n",
    "    cls = LinearSVC()\n",
    "#     print(train_x.shape)\n",
    "    cls.fit(train_x, train_y)\n",
    "    \n",
    "    prediction = []\n",
    "    for idx in test_idx:\n",
    "#         pred = semantic_hard_complaint_rule(chat_data[idx])\n",
    "#         if pred != 1:\n",
    "        pred = cls.predict([chat_encoded[idx]])[0]\n",
    "        prediction.append(pred)\n",
    "#     print(prediction)\n",
    "#     prediction = cls.predict(test_x)\n",
    "    print(classification_report(y_pred= prediction, y_true= test_y))\n",
    "    f1_score.append(classification_report(y_pred= prediction, y_true= test_y, output_dict= True)['micro avg']['f1-score'])\n",
    "    \n",
    "print(\"Average f1-score: {}\".format(np.mean(f1_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average using weight from tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.78      0.83       102\n",
      "           1       0.80      0.90      0.85        99\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       201\n",
      "   macro avg       0.85      0.84      0.84       201\n",
      "weighted avg       0.85      0.84      0.84       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       102\n",
      "           1       0.89      0.92      0.91        99\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       201\n",
      "   macro avg       0.91      0.91      0.91       201\n",
      "weighted avg       0.91      0.91      0.91       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       102\n",
      "           1       0.88      0.90      0.89        99\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       201\n",
      "   macro avg       0.89      0.89      0.89       201\n",
      "weighted avg       0.89      0.89      0.89       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.84       109\n",
      "           1       0.79      0.87      0.83        92\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       201\n",
      "   macro avg       0.84      0.84      0.84       201\n",
      "weighted avg       0.84      0.84      0.84       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.79      0.83       110\n",
      "           1       0.77      0.86      0.81        91\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       201\n",
      "   macro avg       0.82      0.82      0.82       201\n",
      "weighted avg       0.83      0.82      0.82       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89       111\n",
      "           1       0.86      0.89      0.87        90\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       201\n",
      "   macro avg       0.88      0.89      0.88       201\n",
      "weighted avg       0.89      0.89      0.89       201\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.84       100\n",
      "           1       0.82      0.87      0.85       101\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       201\n",
      "   macro avg       0.84      0.84      0.84       201\n",
      "weighted avg       0.84      0.84      0.84       201\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.87       112\n",
      "           1       0.80      0.89      0.84        88\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       200\n",
      "   macro avg       0.85      0.86      0.85       200\n",
      "weighted avg       0.86      0.85      0.86       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.79      0.82       113\n",
      "           1       0.75      0.84      0.79        87\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       200\n",
      "   macro avg       0.81      0.81      0.81       200\n",
      "weighted avg       0.82      0.81      0.81       200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86       105\n",
      "           1       0.81      0.93      0.87        95\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       200\n",
      "   macro avg       0.87      0.87      0.86       200\n",
      "weighted avg       0.87      0.86      0.86       200\n",
      "\n",
      "Average f1-score: 0.8549900497512437\n"
     ]
    }
   ],
   "source": [
    "f1_score = []\n",
    "for train_idx, test_idx in kf.split(chat_data):\n",
    "    train_x, test_x = chat_tf_idf[train_idx], chat_tf_idf[test_idx]\n",
    "    train_y, test_y = chat_label[train_idx], chat_label[test_idx]\n",
    "    cls = LinearSVC()\n",
    "#     print(train_x.shape)\n",
    "    cls.fit(train_x, train_y)\n",
    "    \n",
    "    prediction = []\n",
    "    for idx in test_idx:\n",
    "#         pred = semantic_hard_complaint_rule(chat_data[idx])\n",
    "#         if pred != 1:\n",
    "        pred = cls.predict([chat_tf_idf[idx]])[0]\n",
    "        prediction.append(pred)\n",
    "#     print(prediction)\n",
    "#     prediction = cls.predict(test_x)\n",
    "    print(classification_report(y_pred= prediction, y_true= test_y))\n",
    "    f1_score.append(classification_report(y_pred= prediction, y_true= test_y, output_dict= True)['micro avg']['f1-score'])\n",
    "    \n",
    "print(\"Average f1-score: {}\".format(np.mean(f1_score)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
